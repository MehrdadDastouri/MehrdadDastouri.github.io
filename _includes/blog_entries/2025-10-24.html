<!-- Blog Post: Oct 24, 2025 -->
<article class="p-4 mb-4 border rounded shadow-sm bg-white">
  
  <h2 class="post-title">Deriving the Mean and Variance of the Binomial Distribution</h2>
  
  <p class="post-meta text-muted">A deep dive into one of probability's foundational concepts. | October 24, 2025</p>
  
  <div class="post-content">
    <p>
      The Binomial distribution is a cornerstone of probability theory and statistics, modeling the number of successes in a fixed number of independent trials. For anyone in machine learning, understanding its properties is crucial. Today, let's derive its mean (expected value) and variance from first principles.
    </p>
    <p>
      A random variable $X$ follows a binomial distribution, written as $X \sim B(n, p)$, if its probability mass function (PMF) is:
      $$ P(X=k) = \binom{n}{k} p^k (1-p)^{n-k} $$
      where $n$ is the number of trials, $p$ is the probability of success, and $k$ is the number of successes.
    </p>

    <hr class="my-4">

    <h3>Part 1: Deriving the Mean ($E[X]$)</h3>
    <p>
      The expected value (mean) is defined as $E[X] = \sum_{k=0}^{n} k \cdot P(X=k)$. Let's substitute the PMF:
      $$ E[X] = \sum_{k=0}^{n} k \cdot \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} $$
      The term for $k=0$ is zero, so we can start the sum from $k=1$:
      $$ E[X] = \sum_{k=1}^{n} \frac{n!}{(k-1)!(n-k)!} p^k (1-p)^{n-k} $$
      Now, let's factor out $n$ and $p$:
      $$ E[X] = np \sum_{k=1}^{n} \frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1} (1-p)^{n-k} $$
      Let $m = n-1$ and $j = k-1$. The sum becomes:
      $$ E[X] = np \sum_{j=0}^{m} \frac{m!}{j!(m-j)!} p^{j} (1-p)^{m-j} $$
      The summation part is simply the sum of all probabilities for a new binomial distribution $B(m, p)$, which equals 1. Therefore:
      $$ E[X] = np $$
    </p>

    <h3>Part 2: Deriving the Variance ($\text{Var}(X)$)</h3>
    <p>
      The variance is given by $\text{Var}(X) = E[X^2] - (E[X])^2$. We already have $E[X]$, so we need to find $E[X^2]$. A common trick is to first calculate $E[X(X-1)]$:
      $$ E[X(X-1)] = \sum_{k=0}^{n} k(k-1) \cdot \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} $$
      The terms for $k=0$ and $k=1$ are zero, so we start from $k=2$:
      $$ E[X(X-1)] = \sum_{k=2}^{n} \frac{n!}{(k-2)!(n-k)!} p^k (1-p)^{n-k} $$
      Factoring out $n(n-1)p^2$:
      $$ E[X(X-1)] = n(n-1)p^2 \sum_{k=2}^{n} \frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2} (1-p)^{n-k} $$
      Again, the summation is over a new binomial distribution $B(n-2, p)$, which equals 1. So:
      $$ E[X(X-1)] = n(n-1)p^2 $$
      Now we can find $E[X^2]$ using the relation $E[X^2] = E[X(X-1)] + E[X]$:
      $$ E[X^2] = n(n-1)p^2 + np $$
      Finally, we calculate the variance:
      $$ \text{Var}(X) = (n(n-1)p^2 + np) - (np)^2 $$
      $$ \text{Var}(X) = n^2p^2 - np^2 + np - n^2p^2 $$
      $$ \text{Var}(X) = np - np^2 = np(1-p) $$
    </p>

    <hr class="my-4">

    <h4>Conclusion</h4>
    <p>
      And there we have it. The mean of a binomial distribution is simply $\boldsymbol{np}$, and its variance is $\boldsymbol{np(1-p)}$. These elegant results are fundamental for many statistical methods and machine learning models.
    </p>
  </div>

</article>
